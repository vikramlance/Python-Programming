{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from BeautifulSoup import BeautifulSoup\n",
    "\n",
    "import urllib2\n",
    "url = urllib2.urlopen(\"http://www.python.org\")\n",
    "\n",
    "content = url.read()\n",
    "\n",
    "soup = BeautifulSoup(content)\n",
    "\n",
    "links = soup.findAll(\"a\")\n",
    "\n",
    "#print links\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<title>LinkedIn&#58; Log In or Sign Up\n",
      " </title>\n"
     ]
    }
   ],
   "source": [
    "import cookielib\n",
    "import os\n",
    "import urllib\n",
    "import urllib2\n",
    "import re\n",
    "import string\n",
    "from BeautifulSoup import BeautifulSoup\n",
    "\n",
    "username = \"vikram.lance@email.com\"\n",
    "password = \"online2328711\"\n",
    "\n",
    "cookie_filename = \"parser.cookies.txt\"\n",
    "\n",
    "class LinkedInParser(object):\n",
    "\n",
    "    def __init__(self, login, password):\n",
    "        \"\"\" Start up... \"\"\"\n",
    "        self.login = login\n",
    "        self.password = password\n",
    "\n",
    "        # Simulate browser with cookies enabled\n",
    "        self.cj = cookielib.MozillaCookieJar(cookie_filename)\n",
    "        if os.access(cookie_filename, os.F_OK):\n",
    "            self.cj.load()\n",
    "        self.opener = urllib2.build_opener(\n",
    "            urllib2.HTTPRedirectHandler(),\n",
    "            urllib2.HTTPHandler(debuglevel=0),\n",
    "            urllib2.HTTPSHandler(debuglevel=0),\n",
    "            urllib2.HTTPCookieProcessor(self.cj)\n",
    "        )\n",
    "        self.opener.addheaders = [\n",
    "            ('User-agent', ('Mozilla/4.0 (compatible; MSIE 6.0; '\n",
    "                           'Windows NT 5.2; .NET CLR 1.1.4322)'))\n",
    "        ]\n",
    "\n",
    "        # Login\n",
    "        self.loginPage()\n",
    "\n",
    "        title = self.loadTitle()\n",
    "        print title\n",
    "\n",
    "        self.cj.save()\n",
    "\n",
    "\n",
    "    def loadPage(self, url, data=None):\n",
    "        \"\"\"\n",
    "        Utility function to load HTML from URLs for us with hack to continue despite 404\n",
    "        \"\"\"\n",
    "        # We'll print the url in case of infinite loop\n",
    "        # print \"Loading URL: %s\" % url\n",
    "        try:\n",
    "            if data is not None:\n",
    "                response = self.opener.open(url, data)\n",
    "            else:\n",
    "                response = self.opener.open(url)\n",
    "            return ''.join(response.readlines())\n",
    "        except:\n",
    "            # If URL doesn't load for ANY reason, try again...\n",
    "            # Quick and dirty solution for 404 returns because of network problems\n",
    "            # However, this could infinite loop if there's an actual problem\n",
    "            return self.loadPage(url, data)\n",
    "\n",
    "    def loginPage(self):\n",
    "        \"\"\"\n",
    "        Handle login. This should populate our cookie jar.\n",
    "        \"\"\"\n",
    "        html = self.loadPage(\"https://www.linkedin.com/\")\n",
    "        soup = BeautifulSoup(html)\n",
    "        csrf = soup.find(id=\"loginCsrfParam-login\")['value']\n",
    "\n",
    "        login_data = urllib.urlencode({\n",
    "            'session_key': self.login,\n",
    "            'session_password': self.password,\n",
    "            'loginCsrfParam': csrf,\n",
    "        })\n",
    "\n",
    "        html = self.loadPage(\"https://www.linkedin.com/uas/login-submit\", login_data)\n",
    "        return\n",
    "\n",
    "    def loadTitle(self):\n",
    "        html = self.loadPage(\"http://www.linkedin.com/nhome\")\n",
    "        soup = BeautifulSoup(html)\n",
    "        return soup.find(\"title\")\n",
    "\n",
    "parser = LinkedInParser(username, password)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.linkedin.com/jobs/search/?f_TP=1&amp;keywords;=data%20scientist&amp;location;=United%20States&amp;locationId;=us%3A0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vikra\\Anaconda2\\lib\\site-packages\\bs4\\__init__.py:282: UserWarning: \"https://www.linkedin.com/jobs/search/?f_TP=1&keywords=data%20scientist&location=United%20States&locationId=us%3A0\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n"
     ]
    }
   ],
   "source": [
    "# from BeautifulSoup import BeautifulSoup\n",
    "# import urllib2\n",
    "# import re\n",
    " \n",
    "# html_page = urllib2.urlopen(\"https://www.linkedin.com/jobs/search/?f_TP=1&keywords=data%20scientist&location=United%20States&locationId=us%3A0\")\n",
    "# soup = BeautifulSoup(html_page)\n",
    "# for link in soup.findAll('a', attrs={'href': re.compile(\"^http://\")}):\n",
    "#     print link.get('href')\n",
    "\n",
    "html_doc = \"https://www.linkedin.com/jobs/search/?f_TP=1&keywords=data%20scientist&location=United%20States&locationId=us%3A0\"\n",
    "from bs4 import BeautifulSoup\n",
    "soup = BeautifulSoup(html_doc, 'html.parser')\n",
    "\n",
    "print(soup.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import urllib\n",
    "from BeautifulSoup import BeautifulSoup\n",
    "\n",
    "sock = urllib.urlopen(\"http://stackoverflow.com\")\n",
    "soup = BeautifulSoup(sock.read())\n",
    "sock.close()\n",
    "\n",
    "img = soup.findAll(\"img\")\n",
    "script = soup.findAll(\"script\", {\"type\" : \"text/javascript\"})\n",
    "css = soup.findAll(\"link\", {\"rel\" : \"stylesheet\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
